{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6a81v6ezOZq",
        "outputId": "21c02db8-eb98-425e-d9c8-cbedd9aa704a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry80916NyNLR",
        "outputId": "a1482084-106d-4acf-95b6-4dd62fd6a936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Cited by Affiliations Funding Details Language  Religion_AS  Religion_BU  \\\n",
            "0         0            0               1        0            0            1   \n",
            "1         0            1               0        0            0            1   \n",
            "2         0            0               0        0            0            1   \n",
            "3         0            0               0        0            0            1   \n",
            "4        10            0               0        0            0            1   \n",
            "\n",
            "   Religion_CH  Religion_EA  Religion_IS  Religion_JU  \n",
            "0            0            0            0            0  \n",
            "1            0            0            0            0  \n",
            "2            0            0            0            0  \n",
            "3            0            0            0            0  \n",
            "4            0            0            0            0  \n",
            "['Affiliations', 'Funding Details', 'Language', 'Religion_AS', 'Religion_BU', 'Religion_CH', 'Religion_EA', 'Religion_IS', 'Religion_JU']\n",
            "0.0004047412546978896\n",
            "0.16765924641033053\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,  RepeatedStratifiedKFold\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.metrics import accuracy_score, hamming_loss\n",
        "\n",
        "enc=OneHotEncoder()\n",
        "\n",
        "class DataFrameSelector(BaseEstimator):\n",
        "    \n",
        "    def __init__(self, attribute_names):\n",
        "        self.attribute_names= attribute_names\n",
        "        \n",
        "    def fit(self,X, y = None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        return X[self.attribute_names].values\n",
        "\n",
        "\n",
        "local_path = '/content/'\n",
        "\n",
        "# files = ['bahai', 'buddhism', 'christianity', 'hinduism', 'historical', 'islam','judaism', 'p_j_s', 'shinto', 't_c', 'iranian']\n",
        "files = ['buddhism', 'christianity', 'hinduism', 'islam','judaism', 'p_j_s', 'shinto', 't_c']\n",
        "additions = ['BU', 'CH', 'AS',  'IS', 'JU', 'AS', 'EA', 'EA',]\n",
        "frames = [0]*8\n",
        "results = [0]*8\n",
        "\n",
        "def addColumn(files, additions, frames):\n",
        "    for i in range(len(files)):\n",
        "        csv_path=os.path.join(local_path, files[i] + '.csv')\n",
        "        # results[i] = pd.read_csv(csv_path, nrows=4500)\n",
        "        # print(len(results[i]))\n",
        "        df = pd.read_csv(csv_path, nrows=4500) \n",
        "        df2 = df.assign(Religion=additions[i])\n",
        "        frames[i] = df2\n",
        "\n",
        "# # combine dataframes to one\n",
        "addColumn(files, additions, frames)\n",
        "combined_df = pd.concat(frames, ignore_index=True)\n",
        "combined_df = combined_df.rename(columns={\"Language of Original Document\": \"Language\"})\n",
        "\n",
        "# # IMPUTATION\n",
        "def imputation(combined_df):\n",
        "    north = [\"Hong\", \"Macau\", \"Israel\", \"Japan\", \"Singapore\", \"Korea\", \"Canada\", \"Europe\", \"Russia\", \"Taiwan\", \"Australia\", \"Zealand\", \"Taiwan\", \"US\", \"UK\", \"Germany\", \"France\", \"Spain\", \"Netherlands\", \"Italy\", \"Cyprus\", \"Austria\", \"Belgium\", \"Bulgaria\", \"Croatia\", \"Czech\", \"Denmark\", \"Estonia\", \"Finland\", \"Greece\", \"Hungary\", \"Ireland\", \"Italy\", \"Latvia\", \"Lithuania\", \"Luxembourg\", \"Malta\", \"Poland\", \"Portugal\", \"Romania\", \"Slovakia\", \"Slovenia\", \"Sweden\", \"Iceland\", \"Liechtenstein\", \"Norway\", \"States\", \"Kingdom\", \"Toronto\", \"Switzerland\"]\n",
        "    # western european languages that spread as  a result of colonisation\n",
        "    lang_powerhouses = [\"English\", \"French\", \"Spanish\", \"Portuguese\", \"German\", \"Afrikaans\" ]\n",
        "\n",
        "    # year - 2000 (self feature scaling? - trying to improve results cos 0.18 before)\n",
        "    #trying to drop year ...\n",
        "    # combined_df['Year'] = combined_df['Year'] - 1835\n",
        "\n",
        "#     # replace funding details with 0 if nan, 1 else\n",
        "    combined_df[\"Funding Details\"].where(combined_df[\"Funding Details\"] != combined_df[\"Funding Details\"], 1, inplace = True)\n",
        "    combined_df[\"Funding Details\"].where(combined_df[\"Funding Details\"] == 1, 0, inplace = True)\n",
        "\n",
        "#     # drop column link & column year\n",
        "    combined_df.drop(columns=[\"Link\"], axis=1, inplace=True)\n",
        "    combined_df.drop(columns=[\"Year\"], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "#     # not using get dummies on religion, doing one hot later\n",
        "#     # replace nominal categories (RE) to numerical -> \n",
        "    combined_df = pd.get_dummies(combined_df, columns=[\"Religion\"])\n",
        "\n",
        "#     # remove rows where affilitations is null\n",
        "    combined_df = combined_df.dropna(subset=[\"Affiliations\"])\n",
        "    combined_df.reset_index(drop=True, inplace=True)\n",
        "#     print(len(combined_df))\n",
        "\n",
        "\n",
        "#     # remove punctuation\n",
        "    for i in range(len(combined_df)-1):\n",
        "        combined_df.loc[i+1, \"Affiliations\"] = combined_df.loc[i+1, \"Affiliations\"].translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "\n",
        "#     # if affiliation in north, replace with 0\n",
        "    for i in range (len(north)):\n",
        "        combined_df.Affiliations = combined_df.Affiliations.apply(lambda x: '0' if north[i] in x else x)\n",
        "        # try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "#     # if affiliation not in north, replace with 1\n",
        "    filter  = (combined_df['Affiliations'] != '0' )\n",
        "    combined_df.loc[filter, 'Affiliations'] = 1\n",
        "\n",
        "#     # 0 value from string to int\n",
        "    filter  = (combined_df['Affiliations'] == '0' )\n",
        "    combined_df.loc[filter, 'Affiliations'] = 0\n",
        "#     # datatype is object.. may be incorrect!\n",
        "\n",
        "\n",
        "#     # replace langauge with binary tooo\n",
        "#     # if lang in western powerhouses, replace with 0\n",
        "    for i in range (len(lang_powerhouses)):\n",
        "        combined_df.Language = combined_df.Language.apply(lambda x: '0' if lang_powerhouses[i] in x else x)\n",
        "\n",
        "#     # if lang not in western, replace with 1\n",
        "    filter  = (combined_df['Language'] != '0' )\n",
        "    combined_df.loc[filter, 'Language'] = 1\n",
        "\n",
        "#     # 0 value from string to int\n",
        "    filter  = (combined_df['Language'] == '0' )\n",
        "    combined_df.loc[filter, 'Language'] = 0\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "imputed_df = imputation(combined_df)\n",
        "print(imputed_df.head())\n",
        "train_set, test_set = train_test_split(imputed_df, test_size= 0.3, random_state=42)\n",
        "# # from now on only using train set -> df\n",
        "df=train_set.copy()\n",
        "\n",
        "df['Affiliations'] = df['Affiliations'].astype(int)\n",
        "df['Language'] = df['Language'].astype(int)\n",
        "df['Funding Details'] = df['Funding Details'].astype(int)\n",
        "\n",
        "def fit():\n",
        "  # fit transform uses one hot encoder for all? check..\n",
        "  enc.fit(df[\"Affiliations\"].values.reshape(-1,1))\n",
        "  enc.fit(df[\"Language\"].values.reshape(-1,1))\n",
        "  enc.fit(df[\"Funding Details\"].values.reshape(-1,1))\n",
        "  # enc.fit(df[\"Religion\"].values.reshape(-1,1))\n",
        "\n",
        "def transform():\n",
        "  enc.transform(df[\"Affiliations\"].values.reshape(-1,1)).toarray()\n",
        "  enc.transform(df[\"Language\"].values.reshape(-1,1)).toarray()\n",
        "  enc.transform(df[\"Funding Details\"].values.reshape(-1,1)).toarray()\n",
        "  # enc.transform(df[\"Religion\"].values.reshape(-1,1)).toarray()\n",
        "\n",
        "def fit_transform():\n",
        "  fit()\n",
        "  transform()\n",
        "\n",
        "# # seperate pipelines for numerical and cetegorical data?\n",
        "# # num_attribs=[\"Year\", \"Cited by\"] \n",
        "num_attribs=[\"Cited by\"] \n",
        "cat_attribs=list(df) \n",
        "# # cat_attribs.remove(\"Year\")\n",
        "cat_attribs.remove(\"Cited by\")\n",
        "print(cat_attribs)\n",
        "\n",
        "# test here -> seemed to work indivitually, moves to fit & transform\n",
        "# enc.fit(df[\"Religion\"].values.reshape(-1,1))\n",
        "# print(enc.categories_)\n",
        "# enc.transform(df[\"Religion\"].values.reshape(-1,1)).toarray()\n",
        "\n",
        "# numeric pipeline: scales values\n",
        "num_pipeline= Pipeline([\n",
        "    ('selector', DataFrameSelector(num_attribs)),\n",
        "    # ('imputer',SimpleImputer(strategy=\"median\")),\n",
        "    # ('attribs_adder',CombinedAttributesAdder()),\n",
        "    ('std_scaler',StandardScaler())\n",
        "])\n",
        "\n",
        "# categorical pipeline: encodes \n",
        "cat_pipeline = Pipeline([\n",
        "    ('selector',DataFrameSelector(cat_attribs)),\n",
        "    ('one hot',OneHotEncoder())\n",
        "])\n",
        "\n",
        "\n",
        "full_pipeline = FeatureUnion(transformer_list=[\n",
        "    (\"num_pipeline\",num_pipeline),\n",
        "    (\"cat_pipeline\",cat_pipeline)\n",
        "])\n",
        "\n",
        "\n",
        "data_prepared = full_pipeline.fit_transform(df)\n",
        "data_prepared\n",
        "\n",
        "# set feautures and predictors after fit_transform\n",
        "# religion is goal state -> goal state must be numerical? \n",
        "# data_labels=train_set[\"Year\"].copy()\n",
        "# X = df[[\"Year\", \"Cited by\",\"Language\", \"Affiliations\", \"Funding Details\"]]\n",
        "X = df[[\"Cited by\",\"Language\", \"Affiliations\", \"Funding Details\"]]\n",
        "# # y = df[['Religion_BA', 'Religion_BU', 'Religion_CR', \"Religion_HD\", \"Religion_HI\", \"Religion_IS\", \"Religion_JU\", \"Religion_IN\", \"Religion_JA\", \"Religion_CH\", \"Religion_IR\"]]\n",
        "# y = df[['Religion_IR', 'Religion_BU', 'Religion_CH', 'Religion_AS', 'Religion_HI', 'Religion_IS', 'Religion_JU', 'Religion_EA']]\n",
        "y = df[['Religion_BU', 'Religion_CH', 'Religion_AS', 'Religion_IS', 'Religion_JU', 'Religion_EA']]\n",
        "\n",
        "# X\n",
        "# df.Language.dtype\n",
        "# df.dtypes\n",
        "# # data_prepared.head()\n",
        "# print(data_prepared)\n",
        "# X\n",
        "# X.values\n",
        "# REGRESS ON YEAR - 2000\n",
        "\n",
        "# multiclass classification - Multinomial Naive Bayes\n",
        "# binary_rel_classifier = BinaryRelevance(MultinomialNB())\n",
        "# binary_rel_classifier.fit(X.values,y)\n",
        "\n",
        "# predictions = binary_rel_classifier.predict(X.values)\n",
        "# print(accuracy_score(y,predictions))\n",
        "# print(hamming_loss(y,predictions))\n",
        "\n",
        "# new method - Support Vector Classification\n",
        "# binary_rel_classifier = BinaryRelevance(SVC())\n",
        "# binary_rel_classifier.fit(X.values,y)\n",
        "\n",
        "# predictions = binary_rel_classifier.predict(X.values)\n",
        "# print(accuracy_score(y,predictions))\n",
        "# print(hamming_loss(y,predictions))\n",
        "\n",
        "#SVC\n",
        "# classifier = BinaryRelevance(\n",
        "#     classifier = LinearSVC(),\n",
        "#     require_dense = [False, True]\n",
        "# )\n",
        "\n",
        "# # train\n",
        "# classifier.fit(X.values, y)\n",
        "\n",
        "# # # predict\n",
        "# predictions = classifier.predict(X.values)\n",
        "# print(accuracy_score(y,predictions))\n",
        "# print(hamming_loss(y,predictions))\n",
        "\n",
        "# grid search\n",
        "parameters = [\n",
        "    {\n",
        "        'classifier': [MultinomialNB()],\n",
        "        'classifier__alpha': [0.7, 1.0],\n",
        "    },\n",
        "    # {\n",
        "    #     'classifier': [SVC()],\n",
        "    #     'classifier__kernel': ['rbf', 'linear'],\n",
        "    # },\n",
        "    {\n",
        "        'classifier': [LinearSVC()],\n",
        "        'require_dense': [False, True],\n",
        "    },\n",
        "]\n",
        "\n",
        "# svc = SVC(random_state=0)\n",
        "\n",
        "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=0)\n",
        "\n",
        "# # search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring=\"roc_auc\", cv=cv)\n",
        "\n",
        "clf = GridSearchCV(BinaryRelevance(), parameters, scoring='accuracy')\n",
        "# # cv=cv throws error that data is still multilabel not multiclass\n",
        "clf.fit(X.values, y)\n",
        "\n",
        "# results_df = pd.DataFrame(clf.cv_results_)\n",
        "# results_df = results_df.sort_values(by=[\"rank_test_score\"])\n",
        "# results_df = results_df.set_index(\n",
        "#     results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n",
        "# ).rename_axis(\"kernel\")\n",
        "# results_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]\n",
        "\n",
        "# print(clf.best_params_, clf.best_score_)\n",
        "\n"
      ]
    }
  ]
}